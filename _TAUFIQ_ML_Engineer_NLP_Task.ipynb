{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMswWk+hIlcBdrBA4r28UiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Taufiq-Khan/TAUFIQ-NLP-Task-ML-Headless-Technologies-Limited/blob/main/_TAUFIQ_ML_Engineer_NLP_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dependencies"
      ],
      "metadata": {
        "id": "zt4tNrwA5fpV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkM4Rppz4QHk",
        "outputId": "d222a892-c8db-4114-b939-f31554d2ef67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-22 10:35:45.824359: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import pandas as pd # data manipulation\n",
        "from collections import Counter # count word frequency in a string\n",
        "import re #for regular expression\n",
        "\n",
        "## for data preprocessing\n",
        "import nltk\n",
        "# Download the following 3 (three) packages only 1 time during dependency setup\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords') \n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.porter import PorterStemmer #for steming \n",
        "from nltk.stem import WordNetLemmatizer #for lemmatization\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "\n",
        "## for similarity check\n",
        "import spacy\n",
        "! python -m spacy download en_core_web_lg  #Run only 1 time in terminal during dependency setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. User defined methods to process FAQ data"
      ],
      "metadata": {
        "id": "GlLKaN8x6BKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from dependencies import *\n",
        "\n",
        "# LOAD english stopwords [stpwrd]\n",
        "stpwrd = nltk.corpus.stopwords.words('english') \n",
        "new_stopwords = ['albert', 'einstein', 'date', 'name'] # after self R&D I've found the necessity to consider these words as stopwords\n",
        "stpwrd.extend(new_stopwords) \n",
        "#LOAD spacy package [nlp]\n",
        "nlp = spacy.load(\"en_core_web_lg\") \n",
        "\n",
        "\n",
        "''' # FUNCTION 01: Preprocessing function'''\n",
        "def preprocessing(length_of_df, feature_to_preprocess):\n",
        "    # ps = PorterStemmer()\n",
        "    lemma = WordNetLemmatizer()\n",
        "    preprocessed_text_list = []\n",
        "    for row_i in range(length_of_df):\n",
        "        text = re.sub('[^a-zA-Z]', ' ', str(feature_to_preprocess[row_i]))       # Cleaning symbols, numbers, punctuations, extra spaces, etc.\n",
        "        text = text.lower()                                                      # Converting records into lower case.\n",
        "        text = text.split()\n",
        "        text = [lemma.lemmatize(word) for word in text if not word in stpwrd]    # Reoving stopwords and applying lemmatizattion.\n",
        "        text = ' '.join(text)\n",
        "        preprocessed_text_list.append(text)\n",
        "    return preprocessed_text_list\n",
        "\n",
        "\n",
        "'''# FUNCTION 02: Find hyponyms (specific words such as Hyponyms of Parent are father, mother) of each words in a question'''\n",
        "def hyponyms(word): \n",
        "    hyponyms = []\n",
        "    unnecessary_charecters_front = 8 #there are 8 unnecessary charecters (S y n s e t ( ') before actal hponyms \"Synset('passing.n.02')\"\n",
        "    unnecessary_charecters_last = -7  #there are 6 unnecessary charecters (. n . 0 2 ' ) after actual hponyms \"Synset('passing.n.02')\"\n",
        "    WORD = word.split(' ')\n",
        "    for w in WORD:\n",
        "        synonyms = wn.synsets(w)\n",
        "        if synonyms:\n",
        "            for i in synonyms[0].hyponyms(): # synonyms[0].hyponyms(): hyponyms of only 1st class synonyms have been considered.\n",
        "                i = re.sub('[^a-zA-Z]', ' ', str(i)[unnecessary_charecters_front: unnecessary_charecters_last])\n",
        "                # i initially include Synset('passing.n.02') but actual hypernym/hyponym word is only 'passing'. To get the actual word the above code is written.\n",
        "                # print(i)\n",
        "                hyponyms.append(i)\n",
        "    return hyponyms\n",
        "\n",
        "\n",
        "'''# FUNCTION 03: Find hypernym (generalized or abstract such as hypernym of father can be parent) of each words in a question'''\n",
        "def hypernyms(word):\n",
        "    hypernym = []\n",
        "    unnecessary_charecters_front = 8 #there are 8 unnecessary charecters (S y n s e t ( ') before actal hponyms \"Synset('passing.n.02')\"\n",
        "    unnecessary_charecters_last = -7 #there are 6 unnecessary charecters (. n . 0 2 ' ) after actual hponyms \"Synset('passing.n.02')\"\n",
        "    WORD = word.split(' ')\n",
        "    for w in WORD:\n",
        "        synonyms = wn.synsets(w)\n",
        "        if synonyms:\n",
        "            for i in synonyms[0].hypernyms(): # synonyms[0].hypernyms(): hypernyms of only 1st class synonyms have been considered.\n",
        "                i = re.sub('[^a-zA-Z]', ' ', str(i)[unnecessary_charecters_front: unnecessary_charecters_last])\n",
        "                # i initially include Synset('passing.n.02') but actual hypernym/hyponym word is only 'passing'. To get the actual word the above code is written.\n",
        "                hypernym.append(i)\n",
        "    return hypernym\n",
        "\n",
        "\n",
        "''' # FUNCTION 04: Attach hyponyms and hypernyms with qestion/answer'''\n",
        "def attach_hyperhypo(preprocessed_texts: list, hyperhypo_similarity_threshold: float):\n",
        "    text_with_hyperhypo_list = []\n",
        "    for text in preprocessed_texts:\n",
        "        text_list = text.split(\" \")\n",
        "        hypo = hyponyms(text)                           # find hyponyms\n",
        "        hyper = hypernyms(text)                         # find hypernyms\n",
        "        text_list.extend(hypo)                          # attach hyponyms\n",
        "        text_list.extend(hyper)                         # arrach hypernyms\n",
        "\n",
        "        text_str = (\" \".join (text_list))               # convert text list into string. Such as ['grand father', 'step mother', 'death season' ] -> \"grand father step mother death season\"\n",
        "        text_list_word_by_word = text_str.split(\" \")    #convert text string into list. each list element is single word here. Which reduce redundancy. Such as  \"grand father step mother death season \" -> ['grand', 'father', 'step', 'mother', 'death', 'season']\n",
        "        old_text_list = text.split(\" \")                 # old_text_list will include only base words\n",
        "        new_text_list = []                              # new_text_list will include unique base words, hyponyms and hypernyms of base words\n",
        "        \n",
        "        for base_word in old_text_list:\n",
        "            new_text_list.append(base_word)\n",
        "            for hyperhypo_word in text_list_word_by_word:\n",
        "                if nlp(base_word).similarity(nlp(hyperhypo_word)) > hyperhypo_similarity_threshold:     # if hypernyms and hyponyms have more than 60% similarity with the base words then consider.\n",
        "                    new_text_list.append(hyperhypo_word)\n",
        "\n",
        "        new_text_list = Counter(new_text_list)          # unique words \n",
        "        new_text_str = \" \".join(new_text_list.keys())\n",
        "        text_with_hyperhypo_list.append(new_text_str)\n",
        "    return text_with_hyperhypo_list\n",
        "\n",
        "\n",
        "''' # FUNCTION 05: Create Modified Dataset'''\n",
        "def create_modified_dataset(base_df, new_df_name: str, preprocessed_FAQ: list, preprocessed_FAQ_Ans: list, FAQ_with_hyperhypo: list, FAQ_Ans_with_hyperhypo: list):\n",
        "    df_new = base_df.copy()\n",
        "    len_of_base_df = len(base_df)\n",
        "    df_new['preprocessed_FAQ'], df_new['preprocessed_FAQ_Ans'], df_new['FAQ_with_hyperhypo'], df_new['FAQ_Ans_with_hyperhypo'] = [preprocessed_FAQ, preprocessed_FAQ_Ans, FAQ_with_hyperhypo, FAQ_Ans_with_hyperhypo]\n",
        "    Corpus = list(zip(preprocessed_FAQ, preprocessed_FAQ_Ans))                      # combination of preprocessed FAQs and preprocessed Answers\n",
        "    Corpus = preprocessing(len_of_base_df, Corpus)\n",
        "    Corpus_new = list(zip(FAQ_with_hyperhypo, preprocessed_FAQ_Ans))                # combination of preprocessed FAQs with hypernyms-hyponyms and preprocessed Answers\n",
        "    Corpus_new = preprocessing(len_of_base_df, Corpus_new)\n",
        "    Corpus_with_hyperhypo = list(zip(FAQ_with_hyperhypo,FAQ_Ans_with_hyperhypo ))   # combination of preprocessed FAQs with hypernyms-hyponyms and preprocessed Answers with hypernyms-hyponyms\n",
        "    Corpus_with_hyperhypo = preprocessing(len_of_base_df, Corpus_with_hyperhypo)\n",
        "    df_new['Corpus'] = Corpus\n",
        "    df_new['Corpus_new'] = Corpus_new\n",
        "    df_new['Corpus_with_hyperhypo'] =  Corpus_with_hyperhypo\n",
        "    df_new.to_csv(new_df_name); print('Hybrid dataset creation done')"
      ],
      "metadata": {
        "id": "oXeH2_Hc5-kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. User defined methods to process new test question/ FAQ_test data"
      ],
      "metadata": {
        "id": "kTR7jXXM_n5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to process single test question\n",
        "\n",
        "# from dependencies import *\n",
        "# from functions import hypernyms, hyponyms, nlp, stpwrd\n",
        "\n",
        "\n",
        "'''# FUNCTION 06: Function to preprocess test FAQ'''\n",
        "def preprocessing_single_ques(single_ques_test):\n",
        "    # ps = PorterStemmer()\n",
        "    lemma = WordNetLemmatizer()\n",
        "    question = re.sub('[^a-zA-Z]', ' ', single_ques_test)                           # Cleaning symbols, numbers, punctuations, extra spaces, etc.\n",
        "    question = question.lower()                                                     # Converting records into lower case.\n",
        "    question = question.split()\n",
        "    question = [lemma.lemmatize(word) for word in question if not word in stpwrd]   # Reoving stopwords and applying lemmatizattion\n",
        "    question = ' '.join(question)\n",
        "    return question\n",
        "\n",
        "\n",
        "'''# FUNCTION 07: Attach hypernyms and hyponyms with preprocessed test FAQ text'''\n",
        "def attach_hyperhypo_single(question):\n",
        "    question_list = question.split(\" \")\n",
        "    hypo_of_question = hyponyms(question)                       # find hyponyms\n",
        "    hyper_of_question = hypernyms(question)                     # find hpernyms\n",
        "    question_list.extend(hypo_of_question)                      # attach hyponyms\n",
        "    question_list.extend(hyper_of_question)                     # attach hypernms\n",
        "\n",
        "    question_str = (\" \".join (question_list))                   # convert text list into string. Such as ['grand father', 'step mother', 'death season' ] -> \"grand father step mother death season\"      \n",
        "    question_list_word_by_word = question_str.split(\" \")        #convert text string into list. each list element is single word here. Which reduce redundancy. Such as  \"grand father step mother death season \" -> ['grand', 'father', 'step', 'mother', 'death', 'season']\n",
        "    old_Q_list = question.split(\" \")                            # old_Q_list will include only base words\n",
        "    new_Q_list = []                                             # new_Q_list will include unique base words, hyponyms and hypernyms of base words\n",
        "\n",
        "    for base_q in old_Q_list:\n",
        "        new_Q_list.append(base_q)\n",
        "        for hyperhypo_q in question_list_word_by_word: \n",
        "            if nlp(base_q).similarity(nlp(hyperhypo_q)) > 0.82: # if hypernyms and hyponyms have more than 82% similarity with the base words then consider.\n",
        "                new_Q_list.append(hyperhypo_q)\n",
        "\n",
        "    new_Q_list = Counter(new_Q_list)                            # unique words of a question\n",
        "    new_Q_str = \" \".join(new_Q_list.keys())\n",
        "    return new_Q_str\n",
        "\n",
        "\n",
        "''' # FUNCTION 08: Find most similar FAQ of test FAQ and answer it'''\n",
        "def answer(question_test: str, base_df, base_feature: str ):\n",
        "    question = preprocessing_single_ques(question_test)\n",
        "    question = attach_hyperhypo_single(question)\n",
        "    question_list = question.split(\" \")\n",
        "    question = nlp(question)\n",
        "\n",
        "    most_similar_FAQ_key = -1\n",
        "    similarity_dict = {}                                        # will contain similarity score of each FAQ with test/asked question Also contain iteration number (key) of each FAQ in this Dictionary\n",
        "    similar_FAQ_key = []                                        # will contain key of similar FAQ which has nearly similarity score compared to most similar score key.\n",
        "\n",
        "    for num, faq in enumerate(base_df[base_feature]):\n",
        "        faq_list = faq.split(\" \")\n",
        "        for _ in faq_list:\n",
        "            for __ in question_list:\n",
        "                if _ == __:                                     # if any word of test FAQ is available in base_features FAQ consider the base_features's FAQ as most similar question\n",
        "                    most_similar_FAQ_key = num\n",
        "        similarity_dict[num] = question.similarity(nlp(faq))    # contain similarity score of test FAQ with each base_features's FAQ\n",
        "\n",
        "    if most_similar_FAQ_key != -1:                              # if any word of test FAQ is available in base_features FAQ\n",
        "        most_similar_FAQ = base_df['Question'][most_similar_FAQ_key]    \n",
        "        most_similar_FAQ_Answer = base_df['Answer'][most_similar_FAQ_key]\n",
        "    else:\n",
        "        most_similar_FAQ_score = max(similarity_dict.values())  \n",
        "        most_similar_FAQ_key = max(similarity_dict, key = similarity_dict.get)\n",
        "        \n",
        "        # Near to most similar FAQ\n",
        "        for key, value in similarity_dict.items():\n",
        "            if value >= (most_similar_FAQ_score - 0.1):         # threshold = 0.1: which FAQ's similarity score is not less than (<) most similarity score - 0.1, consider these as almost similar FAQ\n",
        "                similar_FAQ_key.append(key)\n",
        "        similar_FAQ_key.remove(most_similar_FAQ_key)\n",
        "\n",
        "        most_similar_FAQ = base_df['Question'][most_similar_FAQ_key]     \n",
        "        most_similar_FAQ_Answer = base_df['Answer'][most_similar_FAQ_key]     \n",
        "        # print(similarity_dict)\n",
        "        # print(most_similar_FAQ_score)\n",
        "\n",
        "    # print(most_similar_FAQ_key)\n",
        "    print(f\"Asked Question: {question_test}\")\n",
        "    # print(f\"Similar FAQ: {most_similar_FAQ}\")\n",
        "    print(f\"Answer: {most_similar_FAQ_Answer}\", end = ' ')\n",
        "    # For almost similar\n",
        "    if similar_FAQ_key:\n",
        "        for key in similar_FAQ_key:\n",
        "            print(base_df['Answer'][key], end = ' ')    \n",
        "    print()\n",
        "    return most_similar_FAQ_Answer"
      ],
      "metadata": {
        "id": "J1EQnmPA6S-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Process FAQ dataset"
      ],
      "metadata": {
        "id": "2YjGIN-E6nSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from functions import *\n",
        "\n",
        "\"\"\" FETCH DATA \"\"\"\n",
        "FAQs_link = \"https://raw.githubusercontent.com/Muhammad-Taufiq-Khan/TAUFIQ-NLP-Task-ML-Headless-Technologies-Limited/main/FAQs.csv\"\n",
        "df = pd.read_csv(FAQs_link)                                                     ;print(\"Fetched data - FAQ\")\n",
        "\n",
        "\n",
        "\"\"\" PREPROCESSING \"\"\"\n",
        "df_len = len(df)\n",
        "preprocessed_FAQ = preprocessing(df_len, df['Question'])                        ;print(\"Preprocessing done - FAQ\")\n",
        "preprocessed_FAQ_Ans = preprocessing(df_len, df['Answer'])                      ;print(\"Preprocessing done - FAQ Ans\")\n",
        "\n",
        "\n",
        "\"\"\" ATTACHING HYPONYMS AND HYPERNYMS WITH PREPROCESSED TEXTS \"\"\"\n",
        "threshold = 0.6 # hyperhypo similarity threshold\n",
        "FAQ_with_hyperhypo = attach_hyperhypo(preprocessed_FAQ, threshold)             ;print('Attaching hyper-hypo done - FAQ')\n",
        "FAQ_Ans_with_hyperhypo = attach_hyperhypo(preprocessed_FAQ_Ans, threshold)     ;print('Attaching hyper-hypo done - FAQ Ans')\n",
        "\n",
        "\n",
        "\"\"\" CREATING HYBRID DATASET \"\"\"\n",
        "create_modified_dataset(df, 'hybrid_dataset.csv', preprocessed_FAQ, preprocessed_FAQ_Ans, FAQ_with_hyperhypo, FAQ_Ans_with_hyperhypo)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkbKAXh56m12",
        "outputId": "2a05e184-3d08-4c41-8069-d42aeacf3f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched data - FAQ\n",
            "Preprocessing done - FAQ\n",
            "Preprocessing done - FAQ Ans\n",
            "Attaching hyper-hypo done - FAQ\n",
            "Attaching hyper-hypo done - FAQ Ans\n",
            "Hybrid dataset creation done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Test by asking new question or by the FAQ_test dataset"
      ],
      "metadata": {
        "id": "sPRQP66366kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from function_FAQ_test import *\n",
        "modified_df_path = '/content/hybrid_dataset.csv'\n",
        "modified_df = pd.read_csv(modified_df_path)\n",
        "FAQs_test_link = \"https://raw.githubusercontent.com/Muhammad-Taufiq-Khan/TAUFIQ-NLP-Task-ML-Headless-Technologies-Limited/main/FAQs_test.csv\"\n",
        "df_test = pd.read_csv(FAQs_test_link)\n",
        "\n",
        "\n",
        "''' # Test by All FAQ-test with multiple base features '''\n",
        "# base_feature_names = ['preprocessed_FAQ', 'FAQ_with_hyperhypo', 'Corpus', 'Corpus_new', 'Corpus_with_hyperhypo']\n",
        "# for i, base_feature in enumerate(base_feature_names):\n",
        "#     print(f\"\\n #{i+1}. Base Feature: {base_feature}\")\n",
        "#     for question in df_test['Question']:\n",
        "#         Ans = answer(question, modified_df, base_feature); print()\n",
        "\n",
        "\n",
        "''' # Test by all FAQ-test based on best base-feature '''\n",
        "for question in df_test['Question']:\n",
        "        Ans = answer(question, modified_df, 'FAQ_with_hyperhypo'); print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4OQVvP63ux",
        "outputId": "6d1f95fd-2a58-473c-c27f-375c6285754e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asked Question: What is the date of his death?\n",
            "Answer: He was married to Mileva Marić between 1903 and 1919. They had three children, Lieserl (born 1902), Hans Albert (born 1904) and Eduard (born 1910). He married Elsa Löwenthal in 1919 and they lived together until her death in 1936. He died 18 April 1955 in Princeton, New Jersey, USA. His father was Hermann Einstein and his mother was Pauline Einstein (born Koch). He had one sister named Maja. \n",
            "\n",
            "Asked Question: Did Einstein have siblings?\n",
            "Answer: He had one sister named Maja. \n",
            "\n",
            "Asked Question: Who was his wife?\n",
            "Answer: He had one sister named Maja. He was married to Mileva Marić between 1903 and 1919. They had three children, Lieserl (born 1902), Hans Albert (born 1904) and Eduard (born 1910). He married Elsa Löwenthal in 1919 and they lived together until her death in 1936. \n",
            "\n",
            "Asked Question: What was Einstein's father's name?\n",
            "Answer: He had one sister named Maja. His father was Hermann Einstein and his mother was Pauline Einstein (born Koch). \n",
            "\n",
            "Asked Question: At what institutions did he study?\n",
            "Answer: He received his main education at the following schools: Catholic elementary school in Munich, Germany (1885-1888)Luitpold Gymnasium in Munich, Germany (1888-1894) Cantonal school in Aarau, Switzerland (1895-1896) Swiss Federal Institute of Technology in Zurich, Switzerland (1896-1900) Ph.D. from Zurich University, Switzerland (1905) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' # Test by single FAQ-test/ new question based on best base-feature '''\n",
        "question = \"Did Albert Einstein had any baby?\"\n",
        "Ans = answer(question, modified_df, 'FAQ_with_hyperhypo') #;print(f\"\\nAnswer: {Ans}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omyXCbrA8M1t",
        "outputId": "c2c73546-e4fa-4d87-ba34-a68519590918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asked Question: Did Albert Einstein had any baby?\n",
            "Answer: He was married to Mileva Marić between 1903 and 1919. They had three children, Lieserl (born 1902), Hans Albert (born 1904) and Eduard (born 1910). He married Elsa Löwenthal in 1919 and they lived together until her death in 1936. His father was Hermann Einstein and his mother was Pauline Einstein (born Koch). \n"
          ]
        }
      ]
    }
  ]
}